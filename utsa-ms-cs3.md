I have always been drawn towards learning and building. Anytime I learned something I wanted to apply 
it and make something with the new knowledge, including the reverse where I wanted something so I 
learned how to make it. Growing up this evolved into learning about and building various businesses, 
some successfuly and some not so much, but always a learning experience. Not until near the end of my 
undergraduate years did I come to understand the power in computer science and software engineering 
for learning and building alike. Skills that at the basic level are very useful for conducting data 
science tasks and projects, but extend to almost unlimited potential, far beyond just cleaning data 
and training models.

I completed my Bachelors of Science in Statistics and Data Science just last year at UTSA.  During 
this time I was introduced to programs like SPSS, SAS, and R, which at first felt very frustrating as 
I did not fully understand the role that programming played in conducting data focused work.  As time 
went on I got more comfortable with doing the basics in SAS and R, and even learned to appreciate 
these tools for what they allowed me to do.  As I progressed through my classes I developed a new 
view on the world with a focus on the data that makes up our lives, allowing me to look at things in 
a different way but also leading me to new types of questions.  As the questions I wanted answers to 
got more and more complex I was forced to learn more advanced programming, including having to learn 
Python by myself on top of doing my course work in SAS or R as typically required.  My last year of 
school I spent a considerable amount of time learning Python by doing all of my course assignments in 
R, SAS, and Python simultaneously as it allowed me to learn quickly by comparing code I understood in 
R with the equal code in Python.  

After graduating my curiosities only grew for ways to model data and what more could I do with 
Python.  This led me to begin studying machine learning, which at the basic level is equivalent to 
data science.  My prior knowledge of statistics and data science allowed me to quickly get through 
the basics of machine learning as I studied the CS229 Machine Learning and CS230 Deep 
Learning lectures from Stanford, including enrolling in and completing a number of Stanford Online 
courses on ML, DL, and TensorFlow development.  While a lot of my knowledge transferred, over the 
bulk 
of what I had to learn from scratch were concepts like backpropogation, gradient descent, 
other various optimizers, the different network architectures including convolution and 
residual nets, transformers for language as well as ViTs for CV.  Initially 
writing out the math and derivations as many times as I could on 
paper or on white boards, and eventually writing out the concepts in their most raw forms with only 
Python and NumPy.  Putting these skills into practice is when I started learning and developing with 
different ML libraries, first using TensorFlow for a couple months but have since used mostly PyTorch 
the past because it was necessary for me to learn to debug, with it being what HuggingFace libraries 
are built on.      

The recent emergence of LLM's, as well as the majority of established DL research being done more on 
computer vision, influenced my focus on ML/DL study for language.  When I started studying 
transformers and read the Attention Is All You Need paper it made me feel like I felt during 
undergrad where these new concepts made me think about the world in entirely new ways and it made me 
extremely excited.  I started reading every new paper I saw published on SOTA language modeling 
methods and started studying the CS224U Natural Language Understanding lectures from Stanford. 

Moving forward my research interests can largely fall into the ideas of MoEs for foundation 
models, 
distillation and task specific fine tuning paired with distillation to size down large models to 
fit on edge devices, and model interpretability.  


Fine tuning and hardware.  CS concepts becoming more and more important from both a hardware 
persepctive and a low level code perspective.  Learning CUDA.  Learning about floating points and 
their part in making large models more accessible.  

CS faculty researchers.  LLM interpritability.  ML deployment team at Matrix.

Closing    
